{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oB_Cg82JPMI1"
      },
      "source": [
        "# Basic Langchain"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pu_s2Z15HGXw"
      },
      "source": [
        "DOWNLOAD THE LIBRARIES"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-4Te-66oFUam"
      },
      "outputs": [],
      "source": [
        "!pip install -q \\\n",
        "  langchain \\\n",
        "  langchain-google-genai \\\n",
        "  langchain-core \\\n",
        "  langchain-community \\ langgraph"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KgnyPraWIAsZ"
      },
      "source": [
        "Set up the environment keys from secrets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "84asdwd7IEsE"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "\n",
        "os.environ[\"GOOGLE_API_KEY\"] = userdata.get(\"GOOGLE_API_KEY\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "McEHdYb8FG3v"
      },
      "source": [
        "Import the libraries of langchain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YD7GwqNN-os1"
      },
      "outputs": [],
      "source": [
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain import LLMChain\n",
        "from langchain_core.output_parsers import StrOutputParser"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3vJmfXqwEcU2"
      },
      "source": [
        "Create the LLM Model object\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AxSOQJj0EuG_"
      },
      "outputs": [],
      "source": [
        "llm = ChatGoogleGenerativeAI(\n",
        "    model=\"gemini-2.5-flash\",\n",
        "    google_api_key=os.environ[\"GOOGLE_API_KEY\"],\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iLxd2ZUEGsXb"
      },
      "source": [
        "The First Part of Langchain is PROMPT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AFB1PV3UEYqr"
      },
      "outputs": [],
      "source": [
        "prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"You are a helpful technical assistant.\"),\n",
        "    (\"user\", \"Explain this in short: {topic}\")\n",
        "])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JMtgNdxrGwvq"
      },
      "source": [
        "The Second Part of Langchain is CHAIN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "id": "XJqpphU4EaOm",
        "outputId": "0c594a99-ac13-4344-8f0b-725dd635a232"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div style=\"max-width:800px; border: 1px solid var(--colab-border-color);\"><style>\n",
              "      pre.function-repr-contents {\n",
              "        overflow-x: auto;\n",
              "        padding: 8px 12px;\n",
              "        max-height: 500px;\n",
              "      }\n",
              "\n",
              "      pre.function-repr-contents.function-repr-contents-collapsed {\n",
              "        cursor: pointer;\n",
              "        max-height: 100px;\n",
              "      }\n",
              "    </style>\n",
              "    <pre style=\"white-space: initial; background:\n",
              "         var(--colab-secondary-surface-color); padding: 8px 12px;\n",
              "         border-bottom: 1px solid var(--colab-border-color);\"><b>langchain_core.runnables.base.RunnableSequence</b><br/>def __init__(*steps: RunnableLike, name: str | None=None, first: Runnable[Any, Any] | None=None, middle: list[Runnable[Any, Any]] | None=None, last: Runnable[Any, Any] | None=None) -&gt; None</pre><pre class=\"function-repr-contents function-repr-contents-collapsed\" style=\"\"><a class=\"filepath\" style=\"display:none\" href=\"#\">/usr/local/lib/python3.12/dist-packages/langchain_core/runnables/base.py</a>Sequence of `Runnable` objects, where the output of one is the input of the next.\n",
              "\n",
              "**`RunnableSequence`** is the most important composition operator in LangChain\n",
              "as it is used in virtually every chain.\n",
              "\n",
              "A `RunnableSequence` can be instantiated directly or more commonly by using the\n",
              "`|` operator where either the left or right operands (or both) must be a\n",
              "`Runnable`.\n",
              "\n",
              "Any `RunnableSequence` automatically supports sync, async, batch.\n",
              "\n",
              "The default implementations of `batch` and `abatch` utilize threadpools and\n",
              "asyncio gather and will be faster than naive invocation of `invoke` or `ainvoke`\n",
              "for IO bound `Runnable`s.\n",
              "\n",
              "Batching is implemented by invoking the batch method on each component of the\n",
              "`RunnableSequence` in order.\n",
              "\n",
              "A `RunnableSequence` preserves the streaming properties of its components, so if\n",
              "all components of the sequence implement a `transform` method -- which\n",
              "is the method that implements the logic to map a streaming input to a streaming\n",
              "output -- then the sequence will be able to stream input to output!\n",
              "\n",
              "If any component of the sequence does not implement transform then the\n",
              "streaming will only begin after this component is run. If there are\n",
              "multiple blocking components, streaming begins after the last one.\n",
              "\n",
              "!!! note\n",
              "    `RunnableLambdas` do not support `transform` by default! So if you need to\n",
              "    use a `RunnableLambdas` be careful about where you place them in a\n",
              "    `RunnableSequence` (if you need to use the `stream`/`astream` methods).\n",
              "\n",
              "    If you need arbitrary logic and need streaming, you can subclass\n",
              "    Runnable, and implement `transform` for whatever logic you need.\n",
              "\n",
              "Here is a simple example that uses simple functions to illustrate the use of\n",
              "`RunnableSequence`:\n",
              "\n",
              "    ```python\n",
              "    from langchain_core.runnables import RunnableLambda\n",
              "\n",
              "\n",
              "    def add_one(x: int) -&gt; int:\n",
              "        return x + 1\n",
              "\n",
              "\n",
              "    def mul_two(x: int) -&gt; int:\n",
              "        return x * 2\n",
              "\n",
              "\n",
              "    runnable_1 = RunnableLambda(add_one)\n",
              "    runnable_2 = RunnableLambda(mul_two)\n",
              "    sequence = runnable_1 | runnable_2\n",
              "    # Or equivalently:\n",
              "    # sequence = RunnableSequence(first=runnable_1, last=runnable_2)\n",
              "    sequence.invoke(1)\n",
              "    await sequence.ainvoke(1)\n",
              "\n",
              "    sequence.batch([1, 2, 3])\n",
              "    await sequence.abatch([1, 2, 3])\n",
              "    ```\n",
              "\n",
              "Here&#x27;s an example that uses streams JSON output generated by an LLM:\n",
              "\n",
              "    ```python\n",
              "    from langchain_core.output_parsers.json import SimpleJsonOutputParser\n",
              "    from langchain_openai import ChatOpenAI\n",
              "\n",
              "    prompt = PromptTemplate.from_template(\n",
              "        &quot;In JSON format, give me a list of {topic} and their &quot;\n",
              "        &quot;corresponding names in French, Spanish and in a &quot;\n",
              "        &quot;Cat Language.&quot;\n",
              "    )\n",
              "\n",
              "    model = ChatOpenAI()\n",
              "    chain = prompt | model | SimpleJsonOutputParser()\n",
              "\n",
              "    async for chunk in chain.astream({&quot;topic&quot;: &quot;colors&quot;}):\n",
              "        print(&quot;-&quot;)  # noqa: T201\n",
              "        print(chunk, sep=&quot;&quot;, flush=True)  # noqa: T201\n",
              "    ```</pre>\n",
              "      <script>\n",
              "      if (google.colab.kernel.accessAllowed && google.colab.files && google.colab.files.view) {\n",
              "        for (const element of document.querySelectorAll('.filepath')) {\n",
              "          element.style.display = 'block'\n",
              "          element.onclick = (event) => {\n",
              "            event.preventDefault();\n",
              "            event.stopPropagation();\n",
              "            google.colab.files.view(element.textContent, 2811);\n",
              "          };\n",
              "        }\n",
              "      }\n",
              "      for (const element of document.querySelectorAll('.function-repr-contents')) {\n",
              "        element.onclick = (event) => {\n",
              "          event.preventDefault();\n",
              "          event.stopPropagation();\n",
              "          element.classList.toggle('function-repr-contents-collapsed');\n",
              "        };\n",
              "      }\n",
              "      </script>\n",
              "      </div>"
            ],
            "text/plain": [
              "langchain_core.runnables.base.RunnableSequence"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "chain = prompt | llm | StrOutputParser()\n",
        "type(chain)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "beM6TKgIJnIZ"
      },
      "source": [
        "The Third Part of Langchain is TO RUN IT OBIOUSLY"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B61biPDiJwCn",
        "outputId": "0f714451-459b-459f-8acd-f6557e16047d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "A headless CMS is a content management system that focuses *only* on managing and storing content, *without* a built-in frontend or presentation layer (the \"head\").\n",
            "\n",
            "It delivers content purely via APIs (like REST or GraphQL) to *any* separate frontend application or device, offering flexibility to display content across websites, mobile apps, IoT devices, and more.\n"
          ]
        }
      ],
      "source": [
        "response = chain.invoke({\"topic\": \"headless CMS\"})\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RJ68ns4kPiE7"
      },
      "source": [
        "# Agent in Langchain"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "evyyiwAOPnml"
      },
      "source": [
        "First lets import agents\n",
        "and tools\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rXRp5BaiPqW9"
      },
      "outputs": [],
      "source": [
        "from langchain.agents import create_agent\n",
        "from langchain.tools import tool"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QfFrvoMpRcFS"
      },
      "source": [
        "Now Lets Create a tool the Agent can use to answer our prompt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "th8j1X2fRbrh"
      },
      "outputs": [],
      "source": [
        "@tool\n",
        "def fn(val:int) -> int:\n",
        "    \"\"\"It is a function Unknown. Use for math calculations.\"\"\"\n",
        "    return val*val +1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eOMOtp22SFNI"
      },
      "source": [
        "Now Create an agent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wlQRea01SHwJ"
      },
      "outputs": [],
      "source": [
        "agent0 = create_agent(\n",
        "    model=llm,\n",
        "    tools=[fn],\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ikjh7ZJqSJGg"
      },
      "source": [
        "Lets see what the agent can do lel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VAuAsXQbSL3t",
        "outputId": "0a607b40-f04e-49be-d7e9-0553f7c64db7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'messages': [HumanMessage(content='yo what is the function you have as a tool , can u guess it', additional_kwargs={}, response_metadata={}, id='8c244fd8-3913-48f3-8f08-8c2e4970d358'), AIMessage(content=[{'type': 'text', 'text': 'I have a function called `fn`. It takes an integer value as input and is used for mathematical calculations.', 'extras': {'signature': 'CtYBAXLI2nzUQ99KXSQ0bsYK7kzYdT60vRgRw7r/Vanq769ZB0lJPjrtW22Cvpgt9GyeT+zFTqR+a1pLaMwEjS/GL2Ls7BY6hSnrxt1KJl4dp5pJVhjmXDCiGfJF1pJYw1yms4gGy8GJuF3OdlzECKrB+rgBfQD0wjPeen3foqy8lIYtX83tGLZ77YbmwSD0NZEqjRzb0rQPOTQtCechSCP1xVz/ozkeCtXv0WU2zAcrnILRXX7mcI5iks4A0GgAmf0mVZIdggZ0qmFv0xqAoRQSpAsnRnOUFg=='}}], additional_kwargs={}, response_metadata={'finish_reason': 'STOP', 'model_name': 'gemini-2.5-flash', 'safety_ratings': [], 'model_provider': 'google_genai'}, id='lc_run--019c155f-b730-7a52-861b-53f6c8d9aa53-0', tool_calls=[], invalid_tool_calls=[], usage_metadata={'input_tokens': 57, 'output_tokens': 64, 'total_tokens': 121, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 42}})]}\n"
          ]
        }
      ],
      "source": [
        "result1=agent0.invoke({\n",
        "    \"messages\": [  # List of dicts\n",
        "        {\"role\": \"user\", \"content\": \"yo what is the function you have as a tool , can u guess it\"}\n",
        "    ]\n",
        "})\n",
        "\n",
        "print(result1)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3DGPfNkYXyaJ"
      },
      "source": [
        "Trying somethign diff"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YlB4pwNCYAlg"
      },
      "outputs": [],
      "source": [
        "from langgraph.checkpoint.memory import MemorySaver"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0SQyqWouXwdc",
        "outputId": "b771c49c-1cd0-4c3a-d4e8-b880644a1e53"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'messages': [HumanMessage(content='yo what is the function you have as a tool , can u guess it', additional_kwargs={}, response_metadata={}, id='597012b2-7052-484c-a13c-852873346c85'), AIMessage(content=[{'type': 'text', 'text': 'I have a tool function called `fn`. It takes an integer value `val` as input and is described as being for \"math calculations\". Although the specific calculation is unknown, its purpose is clearly stated as mathematical.', 'extras': {'signature': 'CvIEAXLI2nylh1qP98v+C8qO4zC8aUB6bH0zlkJtnpMWnt7uQl40XQy06UdjZ7tr7g8nJ93zAGaztjcFBFElcc/R7oXve5TEfDQikIFBJ8vcZbI0eIemfHewLRft1GumjSF2ZjzEyCfn3D4ZzRwjt51wV2y6ibfWQVYlrgTl+x0DvJzEVbji7kyGgaaSWaAUqH7FYY89uf8+X6R6bPl7OVSueVNjMWI+VO+rWzgVUlhAYjir5Roeq3imQGjIVbOaitHW2iTp0BzopD2PYm7pfyBch0WAR8ywDfALx/AXLvcLjkl/OIzvOSuT2ktgoUzwtkmXiTKdAyfriTiO19tAK4m0w27gaFdBHc2QmGAcbck91mu+PvgCn0dmieEq7E5In8Z9VaiKIFVBkJUAx+ZF85QbfjZt0bRs55cMmjEFB2hwqwnUGDsa0b3lvfnrvKsAjoor/OlmlsZBlAWBc/XC0h8uszlg7wocbwQ426yt/rQ1cfVzywthsjud0GhWrL0gV1J6mxQu8yaGs8Ofi4tyxTJI2MqiznITtjOm91F4TTMv66AI3UfHYzagIYg5NxTLiw7INQfyBq5/EAXZ34BNVECG6o1qBjVghRmp4kRtE5mlA0OxOoL6LgxF3IGajiA3bok2wkGtJ7G50yueLVPaHZYWnSMOIUC4XDprIGlZTj+Wjwg58+T9cuvjb/mWmefYfWpDbw6HAU4doFLPTj/nX4oqF9lZIsquuibp4IieRlLpbto9Aj1DTgw3rlqnp0nBzw0bbTxwrTMUcOvyKQ9kr+7JyxNxb7i5CUEa3gkj5lD+YPu3r0GB+UlvtvROCryhpJfuun4='}}], additional_kwargs={}, response_metadata={'finish_reason': 'STOP', 'model_name': 'gemini-2.5-flash', 'safety_ratings': [], 'model_provider': 'google_genai'}, id='lc_run--019c1562-61b2-7310-848d-b1d060aa4bbb-0', tool_calls=[], invalid_tool_calls=[], usage_metadata={'input_tokens': 57, 'output_tokens': 187, 'total_tokens': 244, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 143}})]}\n"
          ]
        }
      ],
      "source": [
        "checkpointer = MemorySaver()\n",
        "agent = create_agent(model=llm, tools=[fn], checkpointer=checkpointer)\n",
        "config = {\"configurable\": {\"thread_id\": \"math\"}}\n",
        "\n",
        "result1=agent.invoke({\n",
        "    \"messages\": [  # List of dicts\n",
        "        {\"role\": \"user\", \"content\": \"yo what is the function you have as a tool , can u guess it\"}\n",
        "    ]\n",
        "},config)\n",
        "\n",
        "print(result1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gj8Viw53Zca5"
      },
      "source": [
        "# Minimal Reasoning model (for Cyper Cyper)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ftH5B2bYZwvw"
      },
      "source": [
        "I am restarting the session to remove all the variables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xid4fOokZgxD"
      },
      "outputs": [],
      "source": [
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bj8kyFVqcIWo"
      },
      "outputs": [],
      "source": [
        "\n",
        "llm = ChatGoogleGenerativeAI(\n",
        "    model=\"gemini-2.5-flash\",\n",
        "    google_api_key=os.environ[\"GOOGLE_API_KEY\"],\n",
        "    temperature=0\n",
        ")\n",
        "\n",
        "reasoning_prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"\"\"\n",
        "You are an AI support agent for a SaaS platform undergoing a hosted-to-headless migration.\n",
        "\n",
        "Your job:\n",
        "1. Read the observed signals.\n",
        "2. Identify the most likely root cause.\n",
        "3. Recommend ONE clear action.\n",
        "4. State confidence and risk.\n",
        "\n",
        "Be concise but structured.\n",
        "\n",
        "Output format:\n",
        "- Root Cause:\n",
        "- Proposed Action:\n",
        "- Confidence:\n",
        "- Risk:\n",
        "\"\"\"),\n",
        "    (\"user\", \"Here are the observed signals:\\n{analysis}\")\n",
        "])\n",
        "\n",
        "reasoning_chain = reasoning_prompt | llm | StrOutputParser()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b3vWYLQicNE2",
        "outputId": "9fbaddfc-f210-4f20-a218-4122bf892640"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "- Root Cause: The headless API is experiencing performance bottlenecks or resource constraints when processing Stripe webhooks, leading to timeouts and failed checkouts.\n",
            "- Proposed Action: Immediately investigate and optimize the headless API's Stripe webhook handler performance and resource allocation to reduce processing times.\n",
            "- Confidence: High\n",
            "- Risk: Low (Action is diagnostic and optimization-focused, not disruptive.)\n"
          ]
        }
      ],
      "source": [
        "test_analysis = \"\"\"\n",
        "- 12 support tickets complaining: \"checkout keeps failing\"\n",
        "- 30% increase in webhook failures from headless API\n",
        "- Migration stage: 60% completed\n",
        "- Errors mostly from Stripe webhook timeouts\n",
        "\"\"\"\n",
        "\n",
        "answer=reasoning_chain.invoke({\"analysis\": test_analysis})\n",
        "print(answer)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
